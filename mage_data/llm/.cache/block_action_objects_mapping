{"block_file": {"rager/custom/__init__.py:custom:python:rager/custom/  init  ": {"content": "", "file_path": "rager/custom/__init__.py", "language": "python", "type": "custom", "uuid": "rager/custom/__init__"}, "rager/custom/restless_morning.py:custom:python:rager/custom/restless morning": {"content": "if 'custom' not in globals():\n    from mage_ai.data_preparation.decorators import custom\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@custom\ndef transform_custom(*args, **kwargs):\n    \"\"\"\n    args: The output from any upstream parent blocks (if applicable)\n\n    Returns:\n        Anything (e.g. data frame, dictionary, array, int, str, etc.)\n    \"\"\"\n    # Specify your custom logic here\n\n    return {}\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "rager/custom/restless_morning.py", "language": "python", "type": "custom", "uuid": "rager/custom/restless_morning"}, "rager/data_exporters/dreadful_plasma.py:data_exporter:python:rager/data exporters/dreadful plasma": {"content": "from typing import Dict, List, Tuple, Union\n\nimport numpy as np\nfrom elasticsearch import Elasticsearch\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef elasticsearch(\n    documents: List[Dict[str, Union[Dict, List[int], np.ndarray, str]]], *args, **kwargs,\n):\n    \"\"\"\n    Exports document data to an Elasticsearch database.\n    \"\"\"\n\n    connection_string = kwargs.get('connection_string', 'http://localhost:9200')\n    index_name = kwargs.get('index_name', 'documents')\n    number_of_shards = kwargs.get('number_of_shards', 1)\n    number_of_replicas = kwargs.get('number_of_replicas', 0)\n    vector_column_name = kwargs.get('vector_column_name', 'embedding')\n\n    dimensions = kwargs.get('dimensions')\n    if dimensions is None and len(documents) > 0:\n        document = documents[0]\n        dimensions = len(document.get(vector_column_name) or [])\n\n    es_client = Elasticsearch(connection_string)\n\n    print(f'Connecting to Elasticsearch at {connection_string}')\n\n    index_settings = dict(\n        settings=dict(\n            number_of_shards=number_of_shards,\n            number_of_replicas=number_of_replicas,\n        ),\n        mappings=dict(\n            properties=dict(\n                chunk=dict(type='text'),\n                document_id=dict(type='text'),\n                embedding=dict(type='dense_vector', dims=dimensions),\n            ),\n        ),\n    )\n\n    if not es_client.indices.exists(index=index_name):\n        es_client.indices.create(index=index_name)\n        print('Index created with properties:', index_settings)\n        print('Embedding dimensions:', dimensions)\n\n    print(f'Indexing {len(documents)} documents to Elasticsearch index {index_name}')\n    for document in documents:\n        print(f'Indexing document {document[\"document_id\"]}')\n\n        if isinstance(document[vector_column_name], np.ndarray):\n            document[vector_column_name] = document[vector_column_name].tolist()\n\n        es_client.index(index=index_name, document=document)\n", "file_path": "rager/data_exporters/dreadful_plasma.py", "language": "python", "type": "data_exporter", "uuid": "rager/data_exporters/dreadful_plasma"}, "rager/data_exporters/numinous_cyber.py:data_exporter:python:rager/data exporters/numinous cyber": {"content": "from typing import Dict, List, Tuple, Union\n\nimport numpy as np\nfrom elasticsearch import Elasticsearch\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n@data_exporter\ndef elasticsearch(\n    documents: List[Dict[str, Union[Dict, List[int], np.ndarray, str]]], *args, **kwargs,\n):\n    \"\"\"\n    Exports document data to an Elasticsearch database.\n    \"\"\"\n\n    connection_string = kwargs.get('connection_string', 'http://localhost:9200')\n    from datetime import datetime\n\n    index_name_prefix = kwargs.get('index_name', 'documents')\n    current_time = datetime.now().strftime(\"%Y%m%d_%M%S\")\n    index_name = f\"{index_name_prefix}_{current_time}\"\n    print(\"index name:\", index_name)\n    number_of_shards = kwargs.get('number_of_shards', 1)\n    number_of_replicas = kwargs.get('number_of_replicas', 0)\n    vector_column_name = kwargs.get('vector_column_name', 'embedding')\n\n    from mage_ai.data_preparation.variable_manager import set_global_variable\n\n    set_global_variable('cosmic_continuum', 'index_name', index_name) \n\n    dimensions = kwargs.get('dimensions')\n    if dimensions is None and len(documents) > 0:\n        document = documents[0]\n        dimensions = len(document.get(vector_column_name) or [])\n\n    from elasticsearch import Elasticsearch\n\n    es_client = Elasticsearch(\n        connection_string,\n        timeout=60,  # Increase timeout to 60 seconds\n        max_retries=10,\n        retry_on_timeout=True\n    )\n\n\n    print(f'Connecting to Elasticsearch at {connection_string}')\n\n    index_settings = {\n    \"settings\": {\n        \"number_of_shards\": number_of_shards,\n        \"number_of_replicas\": number_of_replicas\n    },\n    \"mappings\": {\n        \"properties\": {\n            \"text\": {\"type\": \"text\"},\n            \"section\": {\"type\": \"text\"},\n            \"question\": {\"type\": \"text\"},\n            \"course\": {\"type\": \"keyword\"},\n            \"document_id\": {\"type\": \"keyword\"}\n        }\n    }\n}\n    if not es_client.indices.exists(index=index_name):\n        es_client.indices.create(index=index_name)\n        print('Index created with properties:', index_settings)\n        print('Embedding dimensions:', dimensions)\n\n    print(f'Indexing {len(documents)} documents to Elasticsearch index {index_name}')\n    for document in documents:\n        print(f'Indexing document {document[\"document_id\"]}')\n\n        es_client.index(index=index_name, document=document)\n        print(f'Index name {index_name}')\n\n        # print(document)", "file_path": "rager/data_exporters/numinous_cyber.py", "language": "python", "type": "data_exporter", "uuid": "rager/data_exporters/numinous_cyber"}, "rager/data_exporters/__init__.py:data_exporter:python:rager/data exporters/  init  ": {"content": "", "file_path": "rager/data_exporters/__init__.py", "language": "python", "type": "data_exporter", "uuid": "rager/data_exporters/__init__"}, "rager/data_exporters/numinous_fission.py:data_exporter:python:rager/data exporters/numinous fission": {"content": "import json\nfrom typing import Dict, List, Tuple, Union\n\nimport numpy as np\nfrom elasticsearch import Elasticsearch\n\n\n@data_exporter\ndef elasticsearch(documents: List[Dict[str, Union[Dict, List[int], str]]], *args, **kwargs):\n    connection_string = kwargs.get('connection_string', 'http://localhost:9200')\n    index_name = kwargs.get('index_name', 'documents')\n    number_of_shards = kwargs.get('number_of_shards', 1)\n    number_of_replicas = kwargs.get('number_of_replicas', 0)\n    dimensions = kwargs.get('dimensions')\n\n    if dimensions is None and len(documents) > 0:\n        document = documents[0]\n        dimensions = len(document.get('embedding') or [])\n\n    es_client = Elasticsearch(connection_string)\n\n    print(f'Connecting to Elasticsearch at {connection_string}')\n\n    index_settings = {\n        \"settings\": {\n            \"number_of_shards\": number_of_shards,\n            \"number_of_replicas\": number_of_replicas,\n        },\n        \"mappings\": {\n            \"properties\": {\n                \"chunk\": {\"type\": \"text\"},\n                \"document_id\": {\"type\": \"text\"},\n                \"embedding\": {\"type\": \"dense_vector\", \"dims\": dimensions}\n            }\n        }\n    }\n\n    # Recreate the index by deleting if it exists and then creating with new settings\n    if es_client.indices.exists(index=index_name):\n        es_client.indices.delete(index=index_name)\n        print(f'Index {index_name} deleted')\n\n    es_client.indices.create(index=index_name, body=index_settings)\n    print('Index created with properties:')\n    print(json.dumps(index_settings, indent=2))\n    print('Embedding dimensions:', dimensions)\n\n    count = len(documents)\n    print(f'Indexing {count} documents to Elasticsearch index {index_name}')\n    for idx, document in enumerate(documents):\n        if idx % 100 == 0:\n\t\t        print(f'{idx + 1}/{count}')\n\n        if isinstance(document['embedding'], np.ndarray):\n            document['embedding'] = document['embedding'].tolist()\n\n        es_client.index(index=index_name, document=document)\n\n    return [[d['embedding'] for d in documents[:10]]]", "file_path": "rager/data_exporters/numinous_fission.py", "language": "python", "type": "data_exporter", "uuid": "rager/data_exporters/numinous_fission"}, "rager/data_loaders/metaphysical_elysium.py:data_loader:python:rager/data loaders/metaphysical elysium": {"content": "from elasticsearch import Elasticsearch, exceptions\nfrom mage_ai.data_preparation.decorators import data_exporter\n\n@data_loader\ndef test_retrieval(*args, **kwargs):\n    index_name = kwargs.get('index_name', 'documents_20240821_*')\n    query = kwargs.get('query', 'When is the next cohort?')\n    connection_string = kwargs.get('connection_string', 'http://localhost:9200')\n    \n    es_client = Elasticsearch([connection_string])\n    \n    search_query = {\n        \"size\": 1,\n        \"query\": {\n            \"match\": {\n                \"text\": query\n            }\n        }\n    }\n    \n    try:\n        response = es_client.search(\n            index=index_name,\n            body=search_query\n        )\n        \n        if response['hits']['hits']:\n            top_result = response['hits']['hits'][0]\n            return top_result['_id']\n        else:\n            return \"No results found.\"\n    \n    except exceptions.NotFoundError as e:\n        return f\"IndexNotFoundError: {e.info}\"\n    except exceptions.RequestError as e:\n        return f\"RequestError: {e.info}\"\n    except exceptions.TransportError as e:\n        return f\"TransportError: {e.info}\"\n    except Exception as e:\n        return f\"Unexpected error: {e}\"\n", "file_path": "rager/data_loaders/metaphysical_elysium.py", "language": "python", "type": "data_loader", "uuid": "rager/data_loaders/metaphysical_elysium"}, "rager/data_loaders/subliminal_phantom.py:data_loader:python:rager/data loaders/subliminal phantom": {"content": "if 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@data_loader\ndef load_data(*args, **kwargs):\n    \"\"\"\n    Template code for loading data from any source.\n\n    Returns:\n        Anything (e.g. data frame, dictionary, array, int, str, etc.)\n    \"\"\"\n    # Specify your data loading logic here\n\n    return {}\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'", "file_path": "rager/data_loaders/subliminal_phantom.py", "language": "python", "type": "data_loader", "uuid": "rager/data_loaders/subliminal_phantom"}, "rager/data_loaders/radiant_nexus.py:data_loader:python:rager/data loaders/radiant nexus": {"content": "import io\nimport requests\nimport docx\nfrom mage_ai.data_preparation.decorators import custom\nfrom mage_ai.data_preparation.models.block import Block\n\ndef clean_line(line):\n    line = line.strip()\n    line = line.strip('\\uFEFF')\n    return line\n\ndef read_faq(file_id):\n    url = f'https://docs.google.com/document/d/{file_id}/export?format=docx'\n    \n    response = requests.get(url)\n    response.raise_for_status()\n    \n    with io.BytesIO(response.content) as f_in:\n        doc = docx.Document(f_in)\n\n    questions = []\n\n    question_heading_style = 'heading 2'\n    section_heading_style = 'heading 1'\n    \n    section_title = ''\n    question_title = ''\n    answer_text_so_far = ''\n     \n    for p in doc.paragraphs:\n        style = p.style.name.lower()\n        p_text = clean_line(p.text)\n    \n        if len(p_text) == 0:\n            continue\n    \n        if style == section_heading_style:\n            section_title = p_text\n            continue\n    \n        if style == question_heading_style:\n            answer_text_so_far = answer_text_so_far.strip()\n            if answer_text_so_far != '' and section_title != '' and question_title != '':\n                questions.append({\n                    'text': answer_text_so_far,\n                    'section': section_title,\n                    'question': question_title,\n                })\n                answer_text_so_far = ''\n    \n            question_title = p_text\n            continue\n        \n        answer_text_so_far += '\\n' + p_text\n    \n    answer_text_so_far = answer_text_so_far.strip()\n    if answer_text_so_far != '' and section_title != '' and question_title != '':\n        questions.append({\n            'text': answer_text_so_far,\n            'section': section_title,\n            'question': question_title,\n        })\n\n    return questions\n\n@custom\ndef ingest_data(*args, **kwargs):\n    faq_documents = {\n        'llm-faq-v1': '1qZjwHkvP0lXHiE4zdbWyUXSVfmVGzougDD6N37bat3E',\n    }\n\n    documents = []\n\n    for course, file_id in faq_documents.items():\n        print(f\"Processing document for course: {course}\")\n        course_documents = read_faq(file_id)\n        documents.append({'course': course, 'documents': course_documents})\n    \n    # Return the processed documents as the output of this block\n    return documents\n", "file_path": "rager/data_loaders/radiant_nexus.py", "language": "python", "type": "data_loader", "uuid": "rager/data_loaders/radiant_nexus"}, "rager/data_loaders/runic_oblivion.py:data_loader:python:rager/data loaders/runic oblivion": {"content": "import requests\nimport json\nfrom typing import List, Dict\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@data_loader\ndef ingest_api_data(*args, **kwargs) -> List[Dict]:\n    \"\"\"\n    Template for loading data from an API.\n    Fetch data from external API using the provided configurations.\n\n    Args:\n        *args: Variable length argument list.\n        **kwargs: Arbitrary keyword arguments.\n\n    Keyword Args:\n        endpoint (str): API endpoint URL.\n        auth_token (str): Authentication token for the API.\n        method (str): HTTP method to use (GET, POST, etc.).\n        timeout (int): Request timeout in seconds.\n    \"\"\"\n    endpoint = kwargs.get('endpoint')\n    auth_token = kwargs.get('auth_token')\n    method = kwargs.get('method', 'GET')\n    timeout = kwargs.get('timeout', 30)\n\n    headers = {}\n    if auth_token:\n        headers['Authorization'] = f\"Bearer {auth_token}\"\n\n    response = requests.request(\n        method=method,\n        url=endpoint or  '',\n        headers=headers,\n        timeout=timeout\n    )\n    response.raise_for_status()\n\n    return [response.json()]\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'", "file_path": "rager/data_loaders/runic_oblivion.py", "language": "python", "type": "data_loader", "uuid": "rager/data_loaders/runic_oblivion"}, "rager/data_loaders/__init__.py:data_loader:python:rager/data loaders/  init  ": {"content": "", "file_path": "rager/data_loaders/__init__.py", "language": "python", "type": "data_loader", "uuid": "rager/data_loaders/__init__"}, "rager/data_loaders/infinite_photon.py:data_loader:python:rager/data loaders/infinite photon": {"content": "from typing import Dict, List, Union\n\nimport numpy as np\nfrom elasticsearch import Elasticsearch\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@data_loader\ndef search(query_embedding: Union[List[int], np.ndarray], *args, **kwargs) -> List[Dict]:\n    connection_string = kwargs.get('connection_string', 'http://localhost:9200')\n    index_name = kwargs.get('index_name', 'documents')\n    source = kwargs.get('source', \"cosineSimilarity(params.query_vector, 'embedding') + 1.0\")\n    top_k = kwargs.get('top_k', 5)\n    chunk_column = kwargs.get('chunk_column', 'content')\n\n    if isinstance(query_embedding, np.ndarray):\n        query_embedding = query_embedding.tolist()\n\n    script_query = dict(\n        script_score=dict(\n            query=dict(match_all=dict()),\n            script=dict(source=source, params=dict(query_vector=query_embedding)),\n        )\n    )\n\n    es_client = Elasticsearch(connection_string)\n\n    response = es_client.search(\n        index=index_name,\n        query=dict(\n            size=top_k,\n            query=script_query,\n            _source=[chunk_column],\n        ),\n    )\n\n    return [hit['_source']['content'] for hit in response['hits']['hits']]\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'", "file_path": "rager/data_loaders/infinite_photon.py", "language": "python", "type": "data_loader", "uuid": "rager/data_loaders/infinite_photon"}, "rager/data_loaders/voltaic_crucible.py:data_loader:python:rager/data loaders/voltaic crucible": {"content": "import io\nimport requests\nimport docx\nfrom mage_ai.data_preparation.decorators import custom\nfrom mage_ai.data_preparation.models.block import Block\n\ndef clean_line(line):\n    line = line.strip()\n    line = line.strip('\\uFEFF')\n    return line\n\ndef read_faq(file_id):\n    url = f'https://docs.google.com/document/d/{file_id}/export?format=docx'\n    \n    response = requests.get(url)\n    response.raise_for_status()\n    \n    with io.BytesIO(response.content) as f_in:\n        doc = docx.Document(f_in)\n\n    questions = []\n\n    question_heading_style = 'heading 2'\n    section_heading_style = 'heading 1'\n    \n    section_title = ''\n    question_title = ''\n    answer_text_so_far = ''\n     \n    for p in doc.paragraphs:\n        style = p.style.name.lower()\n        p_text = clean_line(p.text)\n    \n        if len(p_text) == 0:\n            continue\n    \n        if style == section_heading_style:\n            section_title = p_text\n            continue\n    \n        if style == question_heading_style:\n            answer_text_so_far = answer_text_so_far.strip()\n            if answer_text_so_far != '' and section_title != '' and question_title != '':\n                questions.append({\n                    'text': answer_text_so_far,\n                    'section': section_title,\n                    'question': question_title,\n                })\n                answer_text_so_far = ''\n    \n            question_title = p_text\n            continue\n        \n        answer_text_so_far += '\\n' + p_text\n    \n    answer_text_so_far = answer_text_so_far.strip()\n    if answer_text_so_far != '' and section_title != '' and question_title != '':\n        questions.append({\n            'text': answer_text_so_far,\n            'section': section_title,\n            'question': question_title,\n        })\n\n    return questions\n\n@data_loader\ndef ingest_data(*args, **kwargs):\n    faq_documents = {\n        'llm-faq-v1': '1qZjwHkvP0lXHiE4zdbWyUXSVfmVGzougDD6N37bat3E',\n    }\n\n    documents = []\n\n    for course, file_id in faq_documents.items():\n        print(f\"Processing document for course: {course}\")\n        course_documents = read_faq(file_id)\n        documents.append({'course': course, 'documents': course_documents})\n    \n    # Return the processed documents as the output of this block\n    return documents\n", "file_path": "rager/data_loaders/voltaic_crucible.py", "language": "python", "type": "data_loader", "uuid": "rager/data_loaders/voltaic_crucible"}, "rager/scratchpads/__init__.py:scratchpad:python:rager/scratchpads/  init  ": {"content": "", "file_path": "rager/scratchpads/__init__.py", "language": "python", "type": "scratchpad", "uuid": "rager/scratchpads/__init__"}, "rager/transformers/galvanized_nova.py:transformer:python:rager/transformers/galvanized nova": {"content": "if 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@transformer\ndef transform(data, *args, **kwargs):\n    \"\"\"\n    Template code for a transformer block.\n\n    Add more parameters to this function if this block has multiple parent blocks.\n    There should be one parameter for each output variable from each parent block.\n\n    Args:\n        data: The output from the upstream parent block\n        args: The output from any additional upstream blocks (if applicable)\n\n    Returns:\n        Anything (e.g. data frame, dictionary, array, int, str, etc.)\n    \"\"\"\n    # Specify your transformation logic here\n\n    return data\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'", "file_path": "rager/transformers/galvanized_nova.py", "language": "python", "type": "transformer", "uuid": "rager/transformers/galvanized_nova"}, "rager/transformers/iridescent_hex.py:transformer:python:rager/transformers/iridescent hex": {"content": "if 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\nimport hashlib\n\ndef generate_document_id(doc):\n    combined = f\"{doc['course']}-{doc['question']}-{doc['text'][:10]}\"\n    hash_object = hashlib.md5(combined.encode())\n    hash_hex = hash_object.hexdigest()\n    document_id = hash_hex[:8]\n    return document_id\n\n@transformer\ndef transform(data, *args, **kwargs):\n    documents = []\n\n    for doc in data['documents']:\n        doc['course'] = data['course']\n        # previously we used just \"id\" for document ID\n        doc['document_id'] = generate_document_id(doc)\n        documents.append(doc)\n\n    print(len(documents))\n\n    return documents\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'", "file_path": "rager/transformers/iridescent_hex.py", "language": "python", "type": "transformer", "uuid": "rager/transformers/iridescent_hex"}, "rager/transformers/radiant_photon.py:transformer:python:rager/transformers/radiant photon": {"content": "import re\nfrom typing import Any, Dict, List\n\n\n@transformer\ndef chunk_documents(data: List[Dict[str, Any]], *args, **kwargs):\n    documents = []\n    \n    for idx, item in enumerate(data):\n        course = item['course']\n        \n        for info in item['documents']:\n            section = info['section']\n            question = info['question']\n            answer = info['text']\n            \n            # Generate a unique document ID\n            document_id = ':'.join([re.sub(r'\\W', '_', part) \n\t            for part in [course, section, question]]).lower()\n            \n            # Format the document string\n            chunk = '\\n'.join([\n                f'course:\\n{course}\\n',\n                f'section:\\n{section}\\n',\n                f'question:\\n{question}\\n',\n                f'answer:\\n{answer}\\n',\n            ])\n            \n            documents.append(dict(\n                chunk=chunk,\n                document=info,\n\t            document_id=document_id,\n            ))\n\n    print(f'Documents:', len(documents))\n            \n    return [documents]", "file_path": "rager/transformers/radiant_photon.py", "language": "python", "type": "transformer", "uuid": "rager/transformers/radiant_photon"}, "rager/transformers/vivid_nexus.py:transformer:python:rager/transformers/vivid nexus": {"content": "from typing import Dict, List\n\nimport spacy\n\n\n@transformer\ndef lemmatize_text(documents: List[Dict], *args, **kwargs) -> List[Dict]:\n    count = len(documents)\n    print('Documents', count)\n\n    nlp = spacy.load('en_core_web_sm')\n\n    data = []\n\n    for idx, document in enumerate(documents):\n        document_id = document['document_id']\n        if idx % 100 == 0:\n            print(f'{idx + 1}/{count}')\n\n        # Process the text chunk using spaCy\n        chunk = document['chunk']\n        doc = nlp(chunk)\n        tokens = [token.lemma_ for token in doc]\n\n        data.append(\n            dict(\n                chunk=chunk,\n                document_id=document_id,\n                tokens=tokens,\n            )\n        )\n\n    print('\\nData', len(data))\n\n    return [data]\n", "file_path": "rager/transformers/vivid_nexus.py", "language": "python", "type": "transformer", "uuid": "rager/transformers/vivid_nexus"}, "rager/transformers/__init__.py:transformer:python:rager/transformers/  init  ": {"content": "", "file_path": "rager/transformers/__init__.py", "language": "python", "type": "transformer", "uuid": "rager/transformers/__init__"}, "rager/transformers/prismatic_axiom.py:transformer:python:rager/transformers/prismatic axiom": {"content": "from typing import Dict, List\n\nimport numpy as np\nimport spacy\n\n\n@transformer\ndef spacy_embeddings(documents: List[Dict], *args, **kwargs) -> List[Dict]:\n    count = len(documents)\n    print('Documents', count)\n\n\n    data = []\n    \n    for idx, document in enumerate(documents):\n        document_id = document['document_id']\n        if idx % 100 == 0:\n\t\t        print(f'{idx + 1}/{count}')\n        \n        nlp = spacy.load('en_core_web_sm')\n        tokens = document['tokens']\n    \n        # Combine tokens back into a single string of text used for embedding\n        text = ' '.join(tokens)\n        doc = nlp(text)\n    \n        # Average the word vectors in the doc to get a general embedding\n        embedding = np.mean([token.vector for token in doc], axis=0).tolist()\n    \n        data.append(dict(\n            chunk=document['chunk'],\n            document_id=document['document_id'],\n            embedding=embedding,\n        ))\n    \n    return [data]", "file_path": "rager/transformers/prismatic_axiom.py", "language": "python", "type": "transformer", "uuid": "rager/transformers/prismatic_axiom"}, "rager/pipelines/cosmic_continuum/__init__.py:pipeline:python:rager/pipelines/cosmic continuum/  init  ": {"content": "", "file_path": "rager/pipelines/cosmic_continuum/__init__.py", "language": "python", "type": "pipeline", "uuid": "rager/pipelines/cosmic_continuum/__init__"}, "rager/pipelines/cosmic_continuum/metadata.yaml:pipeline:yaml:rager/pipelines/cosmic continuum/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    file_source:\n      path: rager/data_loaders/voltaic_crucible.py\n  downstream_blocks:\n  - iridescent_hex\n  executor_config: null\n  executor_type: local_python\n  groups:\n  - ingest\n  has_callback: false\n  language: python\n  name: Voltaic crucible\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_loader\n  upstream_blocks: []\n  uuid: voltaic_crucible\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    file_source:\n      path: rager/transformers/iridescent_hex.py\n  downstream_blocks:\n  - numinous_cyber\n  executor_config: null\n  executor_type: local_python\n  groups:\n  - chunking\n  has_callback: false\n  language: python\n  name: Iridescent hex\n  retry_config: null\n  status: executed\n  timeout: null\n  type: transformer\n  upstream_blocks:\n  - voltaic_crucible\n  uuid: iridescent_hex\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    file:\n      content: null\n      extension: py\n      language: python\n      modified_timestamp: 1724229913\n      name: numinous_cyber.py\n      output: null\n      path: /home/src/llm/rager/data_exporters/numinous_cyber.py\n      relative_path: llm/rager/data_exporters/numinous_cyber.py\n      size: 2054\n    file_source:\n      path: rager/data_exporters/numinous_cyber.py\n    templates:\n      elasticsearch:\n        variables:\n          connection_string: http://elasticsearch:9200\n          dimensions: null\n          index_name: documents\n          number_of_replicas: 0\n          number_of_shards: 1\n          vector_column_name: embedding\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  groups:\n  - vector_database\n  has_callback: false\n  language: python\n  name: Numinous cyber\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_exporter\n  upstream_blocks:\n  - iridescent_hex\n  uuid: numinous_cyber\ncache_block_output_in_memory: false\ncallbacks: []\nconcurrency_config: {}\nconditionals: []\ncreated_at: '2024-08-21 08:36:27.020981+00:00'\ndata_integration: null\ndescription: null\nexecution_framework: rag\nexecutor_config: {}\nexecutor_count: 1\nexecutor_type: null\nextensions: {}\nname: Cosmic continuum\nnotification_config: {}\nremote_variables_dir: null\nretry_config: {}\nrun_pipeline_in_one_process: false\nsettings:\n  triggers: null\nspark_config: {}\ntags: []\ntype: python\nuuid: cosmic_continuum\nvariables:\n  index_name: documents_20240821_1406\nvariables_dir: /home/src/mage_data/rager\nwidgets: []\n", "file_path": "rager/pipelines/cosmic_continuum/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "rager/pipelines/cosmic_continuum/metadata"}, "rager/pipelines/cosmic_continuum/interactions.yaml:pipeline:yaml:rager/pipelines/cosmic continuum/interactions": {"content": "blocks: {}\n", "file_path": "rager/pipelines/cosmic_continuum/interactions.yaml", "language": "yaml", "type": "pipeline", "uuid": "rager/pipelines/cosmic_continuum/interactions"}, "rager/pipelines/illuminating_zenith/__init__.py:pipeline:python:rager/pipelines/illuminating zenith/  init  ": {"content": "", "file_path": "rager/pipelines/illuminating_zenith/__init__.py", "language": "python", "type": "pipeline", "uuid": "rager/pipelines/illuminating_zenith/__init__"}, "rager/pipelines/illuminating_zenith/metadata.yaml:pipeline:yaml:rager/pipelines/illuminating zenith/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    file_source:\n      path: rager/data_loaders/subliminal_phantom.py\n  downstream_blocks:\n  - galvanized_nova\n  executor_config: null\n  executor_type: local_python\n  groups:\n  - ingest\n  has_callback: false\n  language: python\n  name: Subliminal phantom\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_loader\n  upstream_blocks: []\n  uuid: subliminal_phantom\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    file_source:\n      path: rager/transformers/galvanized_nova.py\n  downstream_blocks:\n  - dreadful_plasma\n  executor_config: null\n  executor_type: local_python\n  groups:\n  - chunking\n  has_callback: false\n  language: python\n  name: Galvanized nova\n  retry_config: null\n  status: not_executed\n  timeout: null\n  type: transformer\n  upstream_blocks:\n  - subliminal_phantom\n  uuid: galvanized_nova\n- all_upstream_blocks_executed: false\n  color: null\n  configuration:\n    file:\n      content: null\n      extension: py\n      language: python\n      modified_timestamp: 1724232206\n      name: dreadful_plasma.py\n      output: null\n      path: /home/src/llm/rager/data_exporters/dreadful_plasma.py\n      relative_path: llm/rager/data_exporters/dreadful_plasma.py\n      size: 2054\n    file_source:\n      path: rager/data_exporters/dreadful_plasma.py\n    templates:\n      elasticsearch:\n        variables:\n          connection_string: http://elasticsearch:9200\n          dimensions: null\n          index_name: documents\n          number_of_replicas: 0\n          number_of_shards: 1\n          vector_column_name: embedding\n  downstream_blocks:\n  - infinite_photon\n  executor_config: null\n  executor_type: local_python\n  groups:\n  - vector_database\n  has_callback: false\n  language: python\n  name: Dreadful plasma\n  retry_config: null\n  status: failed\n  timeout: null\n  type: data_exporter\n  upstream_blocks:\n  - galvanized_nova\n  uuid: dreadful_plasma\n- all_upstream_blocks_executed: false\n  color: null\n  configuration:\n    file:\n      content: null\n      extension: py\n      language: python\n      modified_timestamp: 1724235456\n      name: infinite_photon.py\n      output: null\n      path: /home/src/llm/rager/data_loaders/infinite_photon.py\n      relative_path: llm/rager/data_loaders/infinite_photon.py\n      size: 1493\n    file_source:\n      path: rager/data_loaders/infinite_photon.py\n    templates:\n      elasticsearch:\n        variables:\n          chunk_column: chunk\n          connection_string: http://elasticsearch:9200\n          index_name: documents\n          vector_column_name: embedding\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  groups:\n  - iterative_retrieval\n  has_callback: false\n  language: python\n  name: Infinite photon\n  retry_config: null\n  status: failed\n  timeout: null\n  type: data_loader\n  upstream_blocks:\n  - dreadful_plasma\n  uuid: infinite_photon\ncache_block_output_in_memory: false\ncallbacks: []\nconcurrency_config: {}\nconditionals: []\ncreated_at: '2024-08-21 09:21:48.798923+00:00'\ndata_integration: null\ndescription: null\nexecution_framework: rag\nexecutor_config: {}\nexecutor_count: 1\nexecutor_type: null\nextensions: {}\nname: Illuminating zenith\nnotification_config: {}\nremote_variables_dir: null\nretry_config: {}\nrun_pipeline_in_one_process: false\nsettings:\n  triggers: null\nspark_config: {}\ntags: []\ntype: python\nuuid: illuminating_zenith\nvariables:\n  index_name: documents_20240821_5016\nvariables_dir: /home/src/mage_data/rager\nwidgets: []\n", "file_path": "rager/pipelines/illuminating_zenith/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "rager/pipelines/illuminating_zenith/metadata"}, "rager/data_loaders/subliminal_phantom.py:data_loader:python:data loaders/subliminal phantom": {"content": "import io\nimport requests\nimport docx\nfrom mage_ai.data_preparation.decorators import custom\nfrom mage_ai.data_preparation.models.block import Block\n\ndef clean_line(line):\n    line = line.strip()\n    line = line.strip('\\uFEFF')\n    return line\n\ndef read_faq(file_id):\n    url = f'https://docs.google.com/document/d/{file_id}/export?format=docx'\n    \n    response = requests.get(url)\n    response.raise_for_status()\n    \n    with io.BytesIO(response.content) as f_in:\n        doc = docx.Document(f_in)\n\n    questions = []\n\n    question_heading_style = 'heading 2'\n    section_heading_style = 'heading 1'\n    \n    section_title = ''\n    question_title = ''\n    answer_text_so_far = ''\n     \n    for p in doc.paragraphs:\n        style = p.style.name.lower()\n        p_text = clean_line(p.text)\n    \n        if len(p_text) == 0:\n            continue\n    \n        if style == section_heading_style:\n            section_title = p_text\n            continue\n    \n        if style == question_heading_style:\n            answer_text_so_far = answer_text_so_far.strip()\n            if answer_text_so_far != '' and section_title != '' and question_title != '':\n                questions.append({\n                    'text': answer_text_so_far,\n                    'section': section_title,\n                    'question': question_title,\n                })\n                answer_text_so_far = ''\n    \n            question_title = p_text\n            continue\n        \n        answer_text_so_far += '\\n' + p_text\n    \n    answer_text_so_far = answer_text_so_far.strip()\n    if answer_text_so_far != '' and section_title != '' and question_title != '':\n        questions.append({\n            'text': answer_text_so_far,\n            'section': section_title,\n            'question': question_title,\n        })\n\n    return questions\n\n@data_loader\ndef ingest_data(*args, **kwargs):\n    faq_documents = {\n        # 'llm-faq-v1': '1qZjwHkvP0lXHiE4zdbWyUXSVfmVGzougDD6N37bat3E',\n        'llm-faq-v1': '1T3MdwUvqCL3jrh3d3VCXQ8xE0UqRzI3bfgpfBq3ZWG0',\n    }\n\n    documents = []\n\n    for course, file_id in faq_documents.items():\n        print(f\"Processing document for course: {course}\")\n        course_documents = read_faq(file_id)\n        documents.append({'course': course, 'documents': course_documents})\n    \n    # Return the processed documents as the output of this block\n    return documents\n", "file_path": "rager/data_loaders/subliminal_phantom.py", "language": "python", "type": "data_loader", "uuid": "subliminal_phantom"}, "rager/data_exporters/dreadful_plasma.py:data_exporter:python:data exporters/dreadful plasma": {"content": "from typing import Dict, List, Tuple, Union\n\nimport numpy as np\nfrom elasticsearch import Elasticsearch\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\nimport requests\n\nresponse = requests.get(\"http://elasticsearch:9200\")\nprint(response.json())\n\n@data_exporter\ndef elasticsearch(\n    documents: List[Dict[str, Union[Dict, List[int], np.ndarray, str]]], *args, **kwargs,\n):\n    \"\"\"\n    Exports document data to an Elasticsearch database.\n    \"\"\"\n\n    connection_string = kwargs.get('connection_string', 'http://localhost:9200')\n    from datetime import datetime\n\n    index_name_prefix = kwargs.get('index_name', 'documents')\n    current_time = datetime.now().strftime(\"%Y%m%d_%M%S\")\n    index_name = f\"{index_name_prefix}_{current_time}\"\n    print(\"index name:\", index_name)\n    number_of_shards = kwargs.get('number_of_shards', 1)\n    number_of_replicas = kwargs.get('number_of_replicas', 0)\n    vector_column_name = kwargs.get('vector_column_name', 'embedding')\n\n    from mage_ai.data_preparation.variable_manager import set_global_variable\n\n    set_global_variable('cosmic_continuum', 'index_name', index_name) \n\n    dimensions = kwargs.get('dimensions')\n    if dimensions is None and len(documents) > 0:\n        document = documents[0]\n        dimensions = len(document.get(vector_column_name) or [])\n\n    from elasticsearch import Elasticsearch\n\n    es_client = Elasticsearch(\n        connection_string,\n        timeout=60,  # Increase timeout to 60 seconds\n        max_retries=10,\n        retry_on_timeout=True\n    )\n\n\n    print(f'Connecting to Elasticsearch at {connection_string}')\n\n    index_settings = {\n    \"settings\": {\n        \"number_of_shards\": number_of_shards,\n        \"number_of_replicas\": number_of_replicas\n    },\n    \"mappings\": {\n        \"properties\": {\n            \"text\": {\"type\": \"text\"},\n            \"section\": {\"type\": \"text\"},\n            \"question\": {\"type\": \"text\"},\n            \"course\": {\"type\": \"keyword\"},\n            \"document_id\": {\"type\": \"keyword\"}\n        }\n    }\n}\n    if not es_client.indices.exists(index=index_name):\n        es_client.indices.create(index=index_name)\n        print('Index created with properties:', index_settings)\n        print('Embedding dimensions:', dimensions)\n\n    print(f'Indexing {len(documents)} documents to Elasticsearch index {index_name}')\n    for document in documents:\n        print(f'Indexing document {document[\"document_id\"]}')\n\n        es_client.index(index=index_name, document=document)\n        print(f'Index name {index_name}')\n\n        # print(document)\n", "file_path": "rager/data_exporters/dreadful_plasma.py", "language": "python", "type": "data_exporter", "uuid": "dreadful_plasma"}, "rager/transformers/galvanized_nova.py:transformer:python:transformers/galvanized nova": {"content": "import hashlib\n\ndef generate_document_id(doc):\n    combined = f\"{doc['course']}-{doc['question']}-{doc['text'][:10]}\"\n    hash_object = hashlib.md5(combined.encode())\n    hash_hex = hash_object.hexdigest()\n    document_id = hash_hex[:8]\n    return document_id\n\n@transformer\ndef chunk_documents(data, *args, **kwargs):\n    documents = []\n\n    for doc in data['documents']:\n        doc['course'] = data['course']\n        # previously we used just \"id\" for document ID\n        doc['document_id'] = generate_document_id(doc)\n        documents.append(doc)\n\n    print(len(documents))\n\n    return documents\n", "file_path": "rager/transformers/galvanized_nova.py", "language": "python", "type": "transformer", "uuid": "galvanized_nova"}, "rager/data_loaders/infinite_photon.py:data_loader:python:data loaders/infinite photon": {"content": "from typing import Dict, List\nfrom elasticsearch import Elasticsearch, exceptions\n\n@data_loader\ndef search(*args, **kwargs) -> List[Dict[str, str]]:\n    \"\"\"\n    Searches for documents in Elasticsearch based on a query.\n    \"\"\"\n    \n    # Default values\n    connection_string = kwargs.get('connection_string', 'http://elasticsearch:9200')\n    index_name = kwargs.get('index_name', 'documents_20240821_1906')  # Ensure index_name has a default value\n    top_k = kwargs.get('top_k', 5)\n    chunk_column = kwargs.get('chunk_column', 'text')\n    \n    # Get the query from args or use a default value\n    query = args[0] if args else \"When is the next cohort?\"\n\n    es_client = Elasticsearch(connection_string)\n    \n    # Construct the search query\n    search_query = {\n        \"size\": top_k,\n        \"query\": {\n            \"match\": {\n                chunk_column: query  # Use chunk_column for flexibility\n            }\n        }\n    }\n    \n    print(\"Sending search query:\", search_query)\n    try:\n        response = es_client.search(\n            index=index_name,\n            body=search_query,\n            _source=[chunk_column]\n        )\n        \n        hits = response['hits']['hits']\n        print(\"Search results:\", hits)\n        \n        return hits\n    \n    except exceptions.BadRequestError as e:\n        print(f\"BadRequestError: {e.info}\")\n        return []\n    except exceptions.ConnectionError as e:\n        print(f\"ConnectionError: {e}\")\n        return []\n    except exceptions.RequestError as e:\n        print(f\"RequestError: {e}\")\n        return []\n    except Exception as e:\n        print(f\"Unexpected error: {e}\")\n        return []\n\n# Example usage\nresults = search(\"When is the next cohort?\", index_name='documents_20240821_1906')\nif results:\n    top_result = results[0]\n    print(f\"Top matching result ID: {top_result['_id']}\")\nelse:\n    print(\"No results found.\")\n", "file_path": "rager/data_loaders/infinite_photon.py", "language": "python", "type": "data_loader", "uuid": "infinite_photon"}}, "custom_block_template": {}, "mage_template": {"data_loaders/deltalake/s3.py:data_loader:python:Amazon S3:Load a Delta Table from Amazon S3.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Amazon S3.", "groups": ["Delta Lake"], "language": "python", "name": "Amazon S3", "path": "data_loaders/deltalake/s3.py"}, "data_loaders/deltalake/azure_blob_storage.py:data_loader:python:Azure Blob Storage:Load a Delta Table from Azure Blob Storage.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Azure Blob Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Azure Blob Storage", "path": "data_loaders/deltalake/azure_blob_storage.py"}, "data_loaders/deltalake/gcs.py:data_loader:python:Google Cloud Storage:Load a Delta Table from Google Cloud Storage.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Google Cloud Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Google Cloud Storage", "path": "data_loaders/deltalake/gcs.py"}, "data_loaders/mongodb.py:data_loader:python:MongoDB:Load data from MongoDB.:Databases (NoSQL)": {"block_type": "data_loader", "description": "Load data from MongoDB.", "groups": ["Databases (NoSQL)"], "language": "python", "name": "MongoDB", "path": "data_loaders/mongodb.py"}, "data_loaders/mssql.py:data_loader:python:MSSQL:Load data from MSSQL.:Databases": {"block_type": "data_loader", "description": "Load data from MSSQL.", "groups": ["Databases"], "language": "python", "name": "MSSQL", "path": "data_loaders/mssql.py"}, "data_exporters/deltalake/s3.py:data_exporter:python:Amazon S3:Export data to a Delta Table in Amazon S3.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Amazon S3.", "groups": ["Delta Lake"], "language": "python", "name": "Amazon S3", "path": "data_exporters/deltalake/s3.py"}, "data_exporters/deltalake/azure_blob_storage.py:data_exporter:python:Azure Blob Storage:Export data to a Delta Table in Azure Blob Storage.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Azure Blob Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Azure Blob Storage", "path": "data_exporters/deltalake/azure_blob_storage.py"}, "data_exporters/deltalake/gcs.py:data_exporter:python:Google Cloud Storage:Export data to a Delta Table in Google Cloud Storage.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Google Cloud Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Google Cloud Storage", "path": "data_exporters/deltalake/gcs.py"}, "data_exporters/mongodb.py:data_exporter:python:MongoDB:Export data to MongoDB.": {"block_type": "data_exporter", "description": "Export data to MongoDB.", "language": "python", "name": "MongoDB", "path": "data_exporters/mongodb.py"}, "data_exporters/mssql.py:data_exporter:python:MSSQL:Export data to MSSQL.:Databases": {"block_type": "data_exporter", "description": "Export data to MSSQL.", "groups": ["Databases"], "language": "python", "name": "MSSQL", "path": "data_exporters/mssql.py"}, "data_loaders/orchestration/triggers/default.jinja:data_loader:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "data_loader", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "data_loaders/orchestration/triggers/default.jinja"}, "data_exporters/orchestration/triggers/default.jinja:data_exporter:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "data_exporter", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "data_exporters/orchestration/triggers/default.jinja"}, "callbacks/base.jinja:callback:python:Base template:Base template with empty functions.": {"block_type": "callback", "description": "Base template with empty functions.", "language": "python", "name": "Base template", "path": "callbacks/base.jinja"}, "callbacks/orchestration/triggers/default.jinja:callback:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "callback", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "callbacks/orchestration/triggers/default.jinja"}, "conditionals/base.jinja:conditional:python:Base template:Base template with empty functions.": {"block_type": "conditional", "description": "Base template with empty functions.", "language": "python", "name": "Base template", "path": "conditionals/base.jinja"}, "data_loaders/default.jinja:data_loader:python:Base template (generic)": {"block_type": "data_loader", "language": "python", "name": "Base template (generic)", "path": "data_loaders/default.jinja"}, "data_loaders/s3.py:data_loader:python:Amazon S3:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "data_loaders/s3.py"}, "data_loaders/azure_blob_storage.py:data_loader:python:Azure Blob Storage:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Azure Blob Storage", "path": "data_loaders/azure_blob_storage.py"}, "data_loaders/google_cloud_storage.py:data_loader:python:Google Cloud Storage:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "data_loaders/google_cloud_storage.py"}, "data_loaders/redshift.py:data_loader:python:Amazon Redshift:Data warehouses": {"block_type": "data_loader", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "data_loaders/redshift.py"}, "data_loaders/bigquery.py:data_loader:python:Google BigQuery:Load data from Google BigQuery.:Data warehouses": {"block_type": "data_loader", "description": "Load data from Google BigQuery.", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "data_loaders/bigquery.py"}, "data_loaders/snowflake.py:data_loader:python:Snowflake:Data warehouses": {"block_type": "data_loader", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "data_loaders/snowflake.py"}, "data_loaders/algolia.py:data_loader:python:Algolia:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Algolia", "path": "data_loaders/algolia.py"}, "data_loaders/chroma.py:data_loader:python:Chroma:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Chroma", "path": "data_loaders/chroma.py"}, "data_loaders/duckdb.py:data_loader:python:DuckDB:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "DuckDB", "path": "data_loaders/duckdb.py"}, "data_loaders/mysql.py:data_loader:python:MySQL:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "data_loaders/mysql.py"}, "data_loaders/oracledb.py:data_loader:python:Oracle DB:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Oracle DB", "path": "data_loaders/oracledb.py"}, "data_loaders/postgres.py:data_loader:python:PostgreSQL:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "data_loaders/postgres.py"}, "data_loaders/qdrant.py:data_loader:python:Qdrant:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Qdrant", "path": "data_loaders/qdrant.py"}, "data_loaders/weaviate.py:data_loader:python:Weaviate:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Weaviate", "path": "data_loaders/weaviate.py"}, "data_loaders/api.py:data_loader:python:API:Fetch data from an API request.": {"block_type": "data_loader", "description": "Fetch data from an API request.", "language": "python", "name": "API", "path": "data_loaders/api.py"}, "data_loaders/file.py:data_loader:python:Local file:Load data from a file on your machine.": {"block_type": "data_loader", "description": "Load data from a file on your machine.", "language": "python", "name": "Local file", "path": "data_loaders/file.py"}, "data_loaders/google_sheets.py:data_loader:python:Google Sheets:Load data from a worksheet in Google Sheets.": {"block_type": "data_loader", "description": "Load data from a worksheet in Google Sheets.", "language": "python", "name": "Google Sheets", "path": "data_loaders/google_sheets.py"}, "data_loaders/druid.py:data_loader:python:Druid": {"block_type": "data_loader", "language": "python", "name": "Druid", "path": "data_loaders/druid.py"}, "transformers/default.jinja:transformer:python:Base template (generic)": {"block_type": "transformer", "language": "python", "name": "Base template (generic)", "path": "transformers/default.jinja"}, "transformers/data_warehouse_transformer.jinja:transformer:python:Amazon Redshift:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "redshift", "data_source_handler": "Redshift"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:Google BigQuery:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "", "data_source": "bigquery", "data_source_handler": "BigQuery"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:Snowflake:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "snowflake", "data_source_handler": "Snowflake"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:PostgreSQL:Databases": {"block_type": "transformer", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "postgres", "data_source_handler": "Postgres"}}, "transformers/transformer_actions/row/drop_duplicate.py:transformer:python:Drop duplicate rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Drop duplicate rows", "path": "transformers/transformer_actions/row/drop_duplicate.py"}, "transformers/transformer_actions/row/filter.py:transformer:python:Filter rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Filter rows", "path": "transformers/transformer_actions/row/filter.py"}, "transformers/transformer_actions/row/remove.py:transformer:python:Remove rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Remove rows", "path": "transformers/transformer_actions/row/remove.py"}, "transformers/transformer_actions/row/sort.py:transformer:python:Sort rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Sort rows", "path": "transformers/transformer_actions/row/sort.py"}, "transformers/transformer_actions/column/average.py:transformer:python:Average value of column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Average value of column", "path": "transformers/transformer_actions/column/average.py"}, "transformers/transformer_actions/column/count_distinct.py:transformer:python:Count unique values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Count unique values in column", "path": "transformers/transformer_actions/column/count_distinct.py"}, "transformers/transformer_actions/column/first.py:transformer:python:First value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "First value in column", "path": "transformers/transformer_actions/column/first.py"}, "transformers/transformer_actions/column/last.py:transformer:python:Last value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Last value in column", "path": "transformers/transformer_actions/column/last.py"}, "transformers/transformer_actions/column/max.py:transformer:python:Maximum value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Maximum value in column", "path": "transformers/transformer_actions/column/max.py"}, "transformers/transformer_actions/column/median.py:transformer:python:Median value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Median value in column", "path": "transformers/transformer_actions/column/median.py"}, "transformers/transformer_actions/column/min.py:transformer:python:Min value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Min value in column", "path": "transformers/transformer_actions/column/min.py"}, "transformers/transformer_actions/column/sum.py:transformer:python:Sum of all values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Sum of all values in column", "path": "transformers/transformer_actions/column/sum.py"}, "transformers/transformer_actions/column/count.py:transformer:python:Total count of values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Total count of values in column", "path": "transformers/transformer_actions/column/count.py"}, "transformers/transformer_actions/column/clean_column_name.py:transformer:python:Clean column name:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Clean column name", "path": "transformers/transformer_actions/column/clean_column_name.py"}, "transformers/transformer_actions/column/fix_syntax_errors.py:transformer:python:Fix syntax errors:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Fix syntax errors", "path": "transformers/transformer_actions/column/fix_syntax_errors.py"}, "transformers/transformer_actions/column/reformat.py:transformer:python:Reformat values in column:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Reformat values in column", "path": "transformers/transformer_actions/column/reformat.py"}, "transformers/transformer_actions/column/select.py:transformer:python:Keep column(s):Column actions:Column removal": {"block_type": "transformer", "groups": ["Column actions", "Column removal"], "language": "python", "name": "Keep column(s)", "path": "transformers/transformer_actions/column/select.py"}, "transformers/transformer_actions/column/remove.py:transformer:python:Remove column(s):Column actions:Column removal": {"block_type": "transformer", "groups": ["Column actions", "Column removal"], "language": "python", "name": "Remove column(s)", "path": "transformers/transformer_actions/column/remove.py"}, "transformers/transformer_actions/column/shift_down.py:transformer:python:Shift row values down:Column actions:Shift": {"block_type": "transformer", "groups": ["Column actions", "Shift"], "language": "python", "name": "Shift row values down", "path": "transformers/transformer_actions/column/shift_down.py"}, "transformers/transformer_actions/column/shift_up.py:transformer:python:Shift row values up:Column actions:Shift": {"block_type": "transformer", "groups": ["Column actions", "Shift"], "language": "python", "name": "Shift row values up", "path": "transformers/transformer_actions/column/shift_up.py"}, "transformers/transformer_actions/column/normalize.py:transformer:python:Normalize data:Column actions:Feature scaling": {"block_type": "transformer", "groups": ["Column actions", "Feature scaling"], "language": "python", "name": "Normalize data", "path": "transformers/transformer_actions/column/normalize.py"}, "transformers/transformer_actions/column/standardize.py:transformer:python:Standardize data:Column actions:Feature scaling": {"block_type": "transformer", "groups": ["Column actions", "Feature scaling"], "language": "python", "name": "Standardize data", "path": "transformers/transformer_actions/column/standardize.py"}, "transformers/transformer_actions/column/impute.py:transformer:python:Fill in missing values:Column actions:Data cleaning": {"block_type": "transformer", "groups": ["Column actions", "Data cleaning"], "language": "python", "name": "Fill in missing values", "path": "transformers/transformer_actions/column/impute.py"}, "transformers/transformer_actions/column/remove_outliers.py:transformer:python:Remove outliers:Column actions:Data cleaning": {"block_type": "transformer", "groups": ["Column actions", "Data cleaning"], "language": "python", "name": "Remove outliers", "path": "transformers/transformer_actions/column/remove_outliers.py"}, "transformers/transformer_actions/column/diff.py:transformer:python:Calculate difference between values:Column actions:Feature extraction": {"block_type": "transformer", "groups": ["Column actions", "Feature extraction"], "language": "python", "name": "Calculate difference between values", "path": "transformers/transformer_actions/column/diff.py"}, "data_exporters/default.jinja:data_exporter:python:Base template (generic)": {"block_type": "data_exporter", "language": "python", "name": "Base template (generic)", "path": "data_exporters/default.jinja"}, "data_exporters/file.py:data_exporter:python:Local file": {"block_type": "data_exporter", "language": "python", "name": "Local file", "path": "data_exporters/file.py"}, "data_exporters/google_sheets.py:data_exporter:python:Google Sheets": {"block_type": "data_exporter", "language": "python", "name": "Google Sheets", "path": "data_exporters/google_sheets.py"}, "data_exporters/s3.py:data_exporter:python:Amazon S3:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "data_exporters/s3.py"}, "data_exporters/azure_blob_storage.py:data_exporter:python:Azure Blob Storage:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Azure Blob Storage", "path": "data_exporters/azure_blob_storage.py"}, "data_exporters/google_cloud_storage.py:data_exporter:python:Google Cloud Storage:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "data_exporters/google_cloud_storage.py"}, "data_exporters/redshift.py:data_exporter:python:Amazon Redshift:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "data_exporters/redshift.py"}, "data_exporters/bigquery.py:data_exporter:python:Google BigQuery:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "data_exporters/bigquery.py"}, "data_exporters/snowflake.py:data_exporter:python:Snowflake:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "data_exporters/snowflake.py"}, "data_exporters/algolia.py:data_exporter:python:Algolia:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Algolia", "path": "data_exporters/algolia.py"}, "data_exporters/chroma.py:data_exporter:python:Chroma:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Chroma", "path": "data_exporters/chroma.py"}, "data_exporters/duckdb.py:data_exporter:python:DuckDB:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "DuckDB", "path": "data_exporters/duckdb.py"}, "data_exporters/mysql.py:data_exporter:python:MySQL:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "data_exporters/mysql.py"}, "data_exporters/oracledb.py:data_exporter:python:OracleDB:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "OracleDB", "path": "data_exporters/oracledb.py"}, "data_exporters/postgres.py:data_exporter:python:PostgreSQL:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "data_exporters/postgres.py"}, "data_exporters/qdrant.py:data_exporter:python:Qdrant:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Qdrant", "path": "data_exporters/qdrant.py"}, "data_exporters/weaviate.py:data_exporter:python:Weaviate:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Weaviate", "path": "data_exporters/weaviate.py"}, "sensors/default.py:sensor:python:Base template (generic)": {"block_type": "sensor", "language": "python", "name": "Base template (generic)", "path": "sensors/default.py"}, "sensors/s3.py:sensor:python:Amazon S3:Data lakes": {"block_type": "sensor", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "sensors/s3.py"}, "sensors/google_cloud_storage.py:sensor:python:Google Cloud Storage:Data lakes": {"block_type": "sensor", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "sensors/google_cloud_storage.py"}, "sensors/redshift.py:sensor:python:Amazon Redshift:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "sensors/redshift.py"}, "sensors/bigquery.py:sensor:python:Google BigQuery:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "sensors/bigquery.py"}, "sensors/snowflake.py:sensor:python:Snowflake:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "sensors/snowflake.py"}, "sensors/mysql.py:sensor:python:MySQL:Databases": {"block_type": "sensor", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "sensors/mysql.py"}, "sensors/postgres.py:sensor:python:PostgreSQL:Databases": {"block_type": "sensor", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "sensors/postgres.py"}, "data_integrations/sources/base:data_loader:sources:Amazon S3:Data integration data loader block for Amazon S3 sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for Amazon S3 sources.", "language": "sources", "name": "Amazon S3", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "Amazon S3"}}, "data_integrations/sources/base:data_loader:sources:Amplitude:Data integration data loader block for Amplitude sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for Amplitude sources.", "language": "sources", "name": "Amplitude", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "Amplitude"}}, "data_integrations/sources/base:data_loader:sources:Api:Data integration data loader block for Api sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for Api sources.", "language": "sources", "name": "Api", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "Api"}}, "data_integrations/sources/base:data_loader:sources:Azure Blob Storage:Data integration data loader block for Azure Blob Storage sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for Azure Blob Storage sources.", "language": "sources", "name": "Azure Blob Storage", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "Azure Blob Storage"}}, "data_integrations/sources/base:data_loader:sources:BigQuery:Data integration data loader block for BigQuery sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for BigQuery sources.", "language": "sources", "name": "BigQuery", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "BigQuery"}}, "data_integrations/sources/base:data_loader:sources:Chargebee:Data integration data loader block for Chargebee sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for Chargebee sources.", "language": "sources", "name": "Chargebee", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "Chargebee"}}, "data_integrations/sources/base:data_loader:sources:Commercetools:Data integration data loader block for Commercetools sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for Commercetools sources.", "language": "sources", "name": "Commercetools", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "Commercetools"}}, "data_integrations/sources/base:data_loader:sources:Couchbase:Data integration data loader block for Couchbase sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for Couchbase sources.", "language": "sources", "name": "Couchbase", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "Couchbase"}}, "data_integrations/sources/base:data_loader:sources:Datadog:Data integration data loader block for Datadog sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for Datadog sources.", "language": "sources", "name": "Datadog", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "Datadog"}}, "data_integrations/sources/base:data_loader:sources:Dremio:Data integration data loader block for Dremio sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for Dremio sources.", "language": "sources", "name": "Dremio", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "Dremio"}}, "data_integrations/sources/base:data_loader:sources:DynamoDB:Data integration data loader block for DynamoDB sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for DynamoDB sources.", "language": "sources", "name": "DynamoDB", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "DynamoDB"}}, "data_integrations/sources/base:data_loader:sources:Facebook Ads:Data integration data loader block for Facebook Ads sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for Facebook Ads sources.", "language": "sources", "name": "Facebook Ads", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "Facebook Ads"}}, "data_integrations/sources/base:data_loader:sources:Freshdesk:Data integration data loader block for Freshdesk sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for Freshdesk sources.", "language": "sources", "name": "Freshdesk", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "Freshdesk"}}, "data_integrations/sources/base:data_loader:sources:Front:Data integration data loader block for Front sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for Front sources.", "language": "sources", "name": "Front", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "Front"}}, "data_integrations/sources/base:data_loader:sources:GitHub:Data integration data loader block for GitHub sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for GitHub sources.", "language": "sources", "name": "GitHub", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "GitHub"}}, "data_integrations/sources/base:data_loader:sources:Google Ads:Data integration data loader block for Google Ads sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for Google Ads sources.", "language": "sources", "name": "Google Ads", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "Google Ads"}}, "data_integrations/sources/base:data_loader:sources:Google Analytics:Data integration data loader block for Google Analytics sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for Google Analytics sources.", "language": "sources", "name": "Google Analytics", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "Google Analytics"}}, "data_integrations/sources/base:data_loader:sources:Google Search Console:Data integration data loader block for Google Search Console sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for Google Search Console sources.", "language": "sources", "name": "Google Search Console", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "Google Search Console"}}, "data_integrations/sources/base:data_loader:sources:Google Sheets:Data integration data loader block for Google Sheets sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for Google Sheets sources.", "language": "sources", "name": "Google Sheets", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "Google Sheets"}}, "data_integrations/sources/base:data_loader:sources:HubSpot:Data integration data loader block for HubSpot sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for HubSpot sources.", "language": "sources", "name": "HubSpot", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "HubSpot"}}, "data_integrations/sources/base:data_loader:sources:Intercom:Data integration data loader block for Intercom sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for Intercom sources.", "language": "sources", "name": "Intercom", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "Intercom"}}, "data_integrations/sources/base:data_loader:sources:Knowi:Data integration data loader block for Knowi sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for Knowi sources.", "language": "sources", "name": "Knowi", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "Knowi"}}, "data_integrations/sources/base:data_loader:sources:LinkedIn Ads:Data integration data loader block for LinkedIn Ads sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for LinkedIn Ads sources.", "language": "sources", "name": "LinkedIn Ads", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "LinkedIn Ads"}}, "data_integrations/sources/base:data_loader:sources:Microsoft SQL Server:Data integration data loader block for Microsoft SQL Server sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for Microsoft SQL Server sources.", "language": "sources", "name": "Microsoft SQL Server", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "Microsoft SQL Server", "uuid": "mssql"}}, "data_integrations/sources/base:data_loader:sources:Mode:Data integration data loader block for Mode sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for Mode sources.", "language": "sources", "name": "Mode", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "Mode"}}, "data_integrations/sources/base:data_loader:sources:Monday:Data integration data loader block for Monday sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for Monday sources.", "language": "sources", "name": "Monday", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "Monday"}}, "data_integrations/sources/base:data_loader:sources:MongoDB:Data integration data loader block for MongoDB sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for MongoDB sources.", "language": "sources", "name": "MongoDB", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "MongoDB"}}, "data_integrations/sources/base:data_loader:sources:MySQL:Data integration data loader block for MySQL sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for MySQL sources.", "language": "sources", "name": "MySQL", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "MySQL"}}, "data_integrations/sources/base:data_loader:sources:OracleDB:Data integration data loader block for OracleDB sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for OracleDB sources.", "language": "sources", "name": "OracleDB", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "OracleDB"}}, "data_integrations/sources/base:data_loader:sources:Outreach:Data integration data loader block for Outreach sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for Outreach sources.", "language": "sources", "name": "Outreach", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "Outreach"}}, "data_integrations/sources/base:data_loader:sources:Paystack:Data integration data loader block for Paystack sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for Paystack sources.", "language": "sources", "name": "Paystack", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "Paystack"}}, "data_integrations/sources/base:data_loader:sources:Pipedrive:Data integration data loader block for Pipedrive sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for Pipedrive sources.", "language": "sources", "name": "Pipedrive", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "Pipedrive"}}, "data_integrations/sources/base:data_loader:sources:PostgreSQL:Data integration data loader block for PostgreSQL sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for PostgreSQL sources.", "language": "sources", "name": "PostgreSQL", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "PostgreSQL"}}, "data_integrations/sources/base:data_loader:sources:Postmark:Data integration data loader block for Postmark sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for Postmark sources.", "language": "sources", "name": "Postmark", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "Postmark"}}, "data_integrations/sources/base:data_loader:sources:PowerBI:Data integration data loader block for PowerBI sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for PowerBI sources.", "language": "sources", "name": "PowerBI", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "PowerBI"}}, "data_integrations/sources/base:data_loader:sources:Redshift:Data integration data loader block for Redshift sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for Redshift sources.", "language": "sources", "name": "Redshift", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "Redshift"}}, "data_integrations/sources/base:data_loader:sources:Salesforce:Data integration data loader block for Salesforce sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for Salesforce sources.", "language": "sources", "name": "Salesforce", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "Salesforce"}}, "data_integrations/sources/base:data_loader:sources:Sftp:Data integration data loader block for Sftp sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for Sftp sources.", "language": "sources", "name": "Sftp", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "Sftp"}}, "data_integrations/sources/base:data_loader:sources:Snowflake:Data integration data loader block for Snowflake sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for Snowflake sources.", "language": "sources", "name": "Snowflake", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "Snowflake"}}, "data_integrations/sources/base:data_loader:sources:Stripe:Data integration data loader block for Stripe sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for Stripe sources.", "language": "sources", "name": "Stripe", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "Stripe"}}, "data_integrations/sources/base:data_loader:sources:Tableau:Data integration data loader block for Tableau sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for Tableau sources.", "language": "sources", "name": "Tableau", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "Tableau"}}, "data_integrations/sources/base:data_loader:sources:Twitter Ads:Data integration data loader block for Twitter Ads sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for Twitter Ads sources.", "language": "sources", "name": "Twitter Ads", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "Twitter Ads"}}, "data_integrations/sources/base:data_loader:sources:Zendesk:Data integration data loader block for Zendesk sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for Zendesk sources.", "language": "sources", "name": "Zendesk", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "Zendesk"}}, "data_integrations/destinations/base:data_exporter:destinations:Amazon S3:Data integration data exporter block for Amazon S3 destinations.": {"block_type": "data_exporter", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data exporter block for Amazon S3 destinations.", "language": "destinations", "name": "Amazon S3", "path": "data_integrations/destinations/base", "template_type": "data_integration", "template_variables": {"name": "Amazon S3"}}, "data_integrations/destinations/base:data_exporter:destinations:BigQuery:Data integration data exporter block for BigQuery destinations.": {"block_type": "data_exporter", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data exporter block for BigQuery destinations.", "language": "destinations", "name": "BigQuery", "path": "data_integrations/destinations/base", "template_type": "data_integration", "template_variables": {"name": "BigQuery"}}, "data_integrations/destinations/base:data_exporter:destinations:Clickhouse:Data integration data exporter block for Clickhouse destinations.": {"block_type": "data_exporter", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data exporter block for Clickhouse destinations.", "language": "destinations", "name": "Clickhouse", "path": "data_integrations/destinations/base", "template_type": "data_integration", "template_variables": {"name": "Clickhouse"}}, "data_integrations/destinations/base:data_exporter:destinations:Delta Lake Azure:Data integration data exporter block for Delta Lake Azure destinations.": {"block_type": "data_exporter", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data exporter block for Delta Lake Azure destinations.", "language": "destinations", "name": "Delta Lake Azure", "path": "data_integrations/destinations/base", "template_type": "data_integration", "template_variables": {"name": "Delta Lake Azure"}}, "data_integrations/destinations/base:data_exporter:destinations:Delta Lake S3:Data integration data exporter block for Delta Lake S3 destinations.": {"block_type": "data_exporter", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data exporter block for Delta Lake S3 destinations.", "language": "destinations", "name": "Delta Lake S3", "path": "data_integrations/destinations/base", "template_type": "data_integration", "template_variables": {"name": "Delta Lake S3"}}, "data_integrations/destinations/base:data_exporter:destinations:Elasticsearch:Data integration data exporter block for Elasticsearch destinations.": {"block_type": "data_exporter", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data exporter block for Elasticsearch destinations.", "language": "destinations", "name": "Elasticsearch", "path": "data_integrations/destinations/base", "template_type": "data_integration", "template_variables": {"name": "Elasticsearch"}}, "data_integrations/destinations/base:data_exporter:destinations:Google Cloud Storage:Data integration data exporter block for Google Cloud Storage destinations.": {"block_type": "data_exporter", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data exporter block for Google Cloud Storage destinations.", "language": "destinations", "name": "Google Cloud Storage", "path": "data_integrations/destinations/base", "template_type": "data_integration", "template_variables": {"name": "Google Cloud Storage"}}, "data_integrations/destinations/base:data_exporter:destinations:Kafka:Data integration data exporter block for Kafka destinations.": {"block_type": "data_exporter", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data exporter block for Kafka destinations.", "language": "destinations", "name": "Kafka", "path": "data_integrations/destinations/base", "template_type": "data_integration", "template_variables": {"name": "Kafka"}}, "data_integrations/destinations/base:data_exporter:destinations:MongoDB:Data integration data exporter block for MongoDB destinations.": {"block_type": "data_exporter", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data exporter block for MongoDB destinations.", "language": "destinations", "name": "MongoDB", "path": "data_integrations/destinations/base", "template_type": "data_integration", "template_variables": {"name": "MongoDB"}}, "data_integrations/destinations/base:data_exporter:destinations:Microsoft SQL Server:Data integration data exporter block for Microsoft SQL Server destinations (MSSQL).": {"block_type": "data_exporter", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data exporter block for Microsoft SQL Server destinations (MSSQL).", "language": "destinations", "name": "Microsoft SQL Server", "path": "data_integrations/destinations/base", "template_type": "data_integration", "template_variables": {"module_name": "MSSQL", "name": "Microsoft SQL Server", "uuid": "mssql"}}, "data_integrations/destinations/base:data_exporter:destinations:MySQL:Data integration data exporter block for MySQL destinations.": {"block_type": "data_exporter", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data exporter block for MySQL destinations.", "language": "destinations", "name": "MySQL", "path": "data_integrations/destinations/base", "template_type": "data_integration", "template_variables": {"name": "MySQL"}}, "data_integrations/destinations/base:data_exporter:destinations:Opensearch:Data integration data exporter block for Opensearch destinations.": {"block_type": "data_exporter", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data exporter block for Opensearch destinations.", "language": "destinations", "name": "Opensearch", "path": "data_integrations/destinations/base", "template_type": "data_integration", "template_variables": {"name": "Opensearch"}}, "data_integrations/destinations/base:data_exporter:destinations:OracleDB:Data integration data exporter block for OracleDB destinations.": {"block_type": "data_exporter", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data exporter block for OracleDB destinations.", "language": "destinations", "name": "OracleDB", "path": "data_integrations/destinations/base", "template_type": "data_integration", "template_variables": {"name": "OracleDB"}}, "data_integrations/destinations/base:data_exporter:destinations:PostgreSQL:Data integration data exporter block for PostgreSQL destinations.": {"block_type": "data_exporter", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data exporter block for PostgreSQL destinations.", "language": "destinations", "name": "PostgreSQL", "path": "data_integrations/destinations/base", "template_type": "data_integration", "template_variables": {"name": "PostgreSQL"}}, "data_integrations/destinations/base:data_exporter:destinations:Redshift:Data integration data exporter block for Redshift destinations.": {"block_type": "data_exporter", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data exporter block for Redshift destinations.", "language": "destinations", "name": "Redshift", "path": "data_integrations/destinations/base", "template_type": "data_integration", "template_variables": {"name": "Redshift"}}, "data_integrations/destinations/base:data_exporter:destinations:Salesforce:Data integration data exporter block for Salesforce destinations.": {"block_type": "data_exporter", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data exporter block for Salesforce destinations.", "language": "destinations", "name": "Salesforce", "path": "data_integrations/destinations/base", "template_type": "data_integration", "template_variables": {"name": "Salesforce"}}, "data_integrations/destinations/base:data_exporter:destinations:Snowflake:Data integration data exporter block for Snowflake destinations.": {"block_type": "data_exporter", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data exporter block for Snowflake destinations.", "language": "destinations", "name": "Snowflake", "path": "data_integrations/destinations/base", "template_type": "data_integration", "template_variables": {"name": "Snowflake"}}, "data_integrations/destinations/base:data_exporter:destinations:Trino:Data integration data exporter block for Trino destinations.": {"block_type": "data_exporter", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data exporter block for Trino destinations.", "language": "destinations", "name": "Trino", "path": "data_integrations/destinations/base", "template_type": "data_integration", "template_variables": {"name": "Trino"}}}}