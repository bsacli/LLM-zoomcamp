{"text": "You can consider several different approaches:\nSampling: In the exploratory phase, you can use random samples of the data.\nChunking: When you do need all the data, you can read and process it in chunks that do fit in the memory.\nOptimizing data types: Pandas\u2019 automatic data type inference (when reading data in) might result in e.g. float64 precision being used to represent integers, which wastes space. You might achieve substantial memory reduction by optimizing the data types.\nUsing Dask, an open-source python project which parallelizes Numpy and Pandas.\n(see, e.g. https://www.vantage-ai.com/en/blog/4-strategies-how-to-deal-with-large-datasets-in-pandas)\nBy Rileen Sinha", "section": "Miscellaneous", "question": "How can I work with very large datasets, e.g. the New York Yellow Taxi dataset, with over a million rows?", "course": "machine-learning-zoomcamp", "document_id": "d6f0c6ea"}