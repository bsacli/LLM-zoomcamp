{"text": "Solution:\nIf you\u2019re using Mage, in the last Data Exporter that writes to Google Cloud Storage use PyArrow to generate the Parquet file with the correct logical type for the datetime columns, otherwise they won't be converted to timestamp when loaded by BigQuery later on.\nimport pyarrow as pa\nimport pyarrow.parquet as pq\nimport os\nif 'data_exporter' not in globals():\nfrom mage_ai.data_preparation.decorators import data_exporter\n# Replace with the location of your service account key JSON file.\nos.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '/home/src/personal-gcp.json'\nbucket_name = \"<YOUR_BUCKET_NAME>\"\nobject_key = 'nyc_taxi_data_2022.parquet'\nwhere = f'{bucket_name}/{object_key}'\n@data_exporter\ndef export_data(data, *args, **kwargs):\ntable = pa.Table.from_pandas(data, preserve_index=False)\ngcs = pa.fs.GcsFileSystem()\npq.write_table(\ntable,\nwhere,\n# Convert integer columns in Epoch milliseconds\n# to Timestamp columns in microseconds ('us') so\n# they can be loaded into BigQuery with the right\n# data type\ncoerce_timestamps='us',\nfilesystem=gcs\n)\nSolution 2:\nIf you\u2019re using Mage, in the last Data Exporter that writes to Google Cloud Storage, provide PyArrow with explicit schema to generate the Parquet file with the correct logical type for the datetime columns, otherwise they won't be converted to timestamp when loaded by BigQuery later on.\nschema = pa.schema([\n('vendor_id', pa.int64()),\n('lpep_pickup_datetime', pa.timestamp('ns')),\n('lpep_dropoff_datetime', pa.timestamp('ns')),\n('store_and_fwd_flag', pa.string()),\n('ratecode_id', pa.int64()),\n('pu_location_id', pa.int64()),\n('do_location_id', pa.int64()),\n('passenger_count', pa.int64()),\n('trip_distance', pa.float64()),\n('fare_amount', pa.float64()),\n('extra', pa.float64()),\n('mta_tax', pa.float64()),\n('tip_amount', pa.float64()),\n('tolls_amount', pa.float64()),\n('improvement_surcharge', pa.float64()),\n('total_amount', pa.float64()),\n('payment_type', pa.int64()),\n('trip_type', pa.int64()),\n('congestion_surcharge', pa.float64()),\n('lpep_pickup_month', pa.int64())\n])\ntable = pa.Table.from_pandas(data, schema=schema)", "section": "Module 3: Data Warehousing", "question": "GCP BQ - Datetime columns in Parquet files created from Pandas show up as integer columns in BigQuery", "course": "data-engineering-zoomcamp", "document_id": "0516ccbe"}