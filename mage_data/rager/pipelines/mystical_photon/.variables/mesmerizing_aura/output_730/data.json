{"text": "If you are running tensorflow on your own machine and you start getting the following errors:\nAllocator (GPU_0_bfc) ran out of memory trying to allocate 6.88GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\nTry adding this code in a cell at the beginning of your notebook:\nconfig = tf.compat.v1.ConfigProto()\nconfig.gpu_options.allow_growth = True\nsession = tf.compat.v1.Session(config=config)\nAfter doing this most of my issues went away. I say most because there was one instance when I still got the error once more, but only during one epoch. I ran the code again, right after it finished, and I never saw the error again.\nAdded by Martin Uribe", "section": "10. Kubernetes and TensorFlow Serving", "question": "Getting: Allocator ran out of memory errors?", "course": "machine-learning-zoomcamp", "document_id": "a64aed6b"}