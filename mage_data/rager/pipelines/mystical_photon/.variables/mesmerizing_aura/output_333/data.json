{"text": "Link to Slack Thread : has anyone figured out how to read from GCP data lake instead of downloading all the taxi data again?\nThere\u2019s a few extra steps to go into reading from GCS with PySpark\n1.)  IMPORTANT: Download the Cloud Storage connector for Hadoop here: https://cloud.google.com/dataproc/docs/concepts/connectors/cloud-storage#clusters\nAs the name implies, this .jar file is what essentially connects PySpark with your GCS\n2.) Move the .jar file to your Spark file directory. I installed Spark using homebrew on my MacOS machine and I had to create a /jars directory under \"/opt/homebrew/Cellar/apache-spark/3.2.1/ (where my spark dir is located)\n3.) In your Python script, there are a few extra classes you\u2019ll have to import:\nimport pyspark\nfrom pyspark.sql import SparkSession\nfrom pyspark.conf import SparkConf\nfrom pyspark.context import SparkContext\n4.) You must set up your configurations before building your SparkSession. Here\u2019s my code snippet:\nconf = SparkConf() \\\n.setMaster('local[*]') \\\n.setAppName('test') \\\n.set(\"spark.jars\", \"/opt/homebrew/Cellar/apache-spark/3.2.1/jars/gcs-connector-hadoop3-latest.jar\") \\\n.set(\"spark.hadoop.google.cloud.auth.service.account.enable\", \"true\") \\\n.set(\"spark.hadoop.google.cloud.auth.service.account.json.keyfile\", \"path/to/google_credentials.json\")\nsc = SparkContext(conf=conf)\nsc._jsc.hadoopConfiguration().set(\"fs.AbstractFileSystem.gs.impl\",  \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\nsc._jsc.hadoopConfiguration().set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\nsc._jsc.hadoopConfiguration().set(\"fs.gs.auth.service.account.json.keyfile\", \"path/to/google_credentials.json\")\nsc._jsc.hadoopConfiguration().set(\"fs.gs.auth.service.account.enable\", \"true\")\n5.) Once you run that, build your SparkSession with the new parameters we\u2019d just instantiated in the previous step:\nspark = SparkSession.builder \\\n.config(conf=sc.getConf()) \\\n.getOrCreate()\n6.) Finally, you\u2019re able to read your files straight from GCS!\ndf_green = spark.read.parquet(\"gs://{BUCKET}/green/202*/\")", "section": "Module 5: pyspark", "question": "Spark Cloud Storage connector", "course": "data-engineering-zoomcamp", "document_id": "50c009ef"}