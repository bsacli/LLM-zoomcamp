{"text": "If anyone is a Pythonista or becoming one (which you will essentially be one along this journey), and desires to have all python dependencies under same virtual environment (e.g. conda) as done with prefect and previous exercises, simply follow these steps\nInstall OpenJDK 11,\non MacOS: $ brew install java11\nAdd export PATH=\"/opt/homebrew/opt/openjdk@11/bin:$PATH\"\nto ~/.bashrc or ~/zshrc\nActivate working environment (by pipenv / poetry / conda)\nRun $ pip install pyspark\nWork with exercises as normal\nAll default commands of spark will be also available at shell session under activated enviroment.\nHope this can help!\nP.s. you won\u2019t need findspark to firstly initialize.\nPy4J - Py4JJavaError: An error occurred while calling (...)  java.net.ConnectException: Connection refused: no further information;\nIf you're getting `Py4JavaError` with a generic root cause, such as the described above (Connection refused: no further information). You're most likely using incompatible versions of the JDK or Python with Spark.\nAs of the current latest Spark version (3.5.0), it supports JDK 8 / 11 / 17. All of which can be easily installed with SDKMan! on macOS or Linux environments\n\n$ sdk install java 17.0.10-librca\n$ sdk install spark 3.5.0\n$ sdk install hadoop 3.3.5\nAs PySpark 3.5.0 supports Python 3.8+ make sure you're setting up your virtualenv with either 3.8 / 3.9 / 3.10 / 3.11 (Most importantly avoid using 3.12 for now as not all libs in the data-science/engineering ecosystem are fully package for that)\n\n\n$ conda create -n ENV_NAME python=3.11\n$ conda activate ENV_NAME\n$ pip install pyspark==3.5.0\nThis setup makes installing `findspark` and the likes of it unnecessary. Happy coding.\nPy4J - Py4JJavaError: An error occurred while calling o54.parquet. Or any kind of Py4JJavaError that show up after run df.write.parquet('zones')(On window)\nThis assume you already correctly set up the PATH in the nano ~/.bashrc\nHere my\nexport JAVA_HOME=\"/c/tools/jdk-11.0.21\"\nexport PATH=\"${JAVA_HOME}/bin:${PATH}\"\nexport HADOOP_HOME=\"/c/tools/hadoop-3.2.0\"\nexport PATH=\"${HADOOP_HOME}/bin:${PATH}\"\nexport SPARK_HOME=\"/c/tools/spark-3.3.2-bin-hadoop3\"\nexport PATH=\"${SPARK_HOME}/bin:${PATH}\"\nexport PYTHONPATH=\"${SPARK_HOME}/python/:$PYTHONPATH\"\nexport PYTHONPATH=\"${SPARK_HOME}spark-3.5.1-bin-hadoop3py4j-0.10.9.5-src.zip:$PYTHONPATH\"\nYou also need to add environment variables correctly which paths to java jdk, spark and hadoop through\nGo to Stephenlaye2/winutils3.3.0: winutils.exe hadoop.dll and hdfs.dll binaries for hadoop windows (github.com), download the right winutils for hadoop-3.2.0. Then create a new folder,bin and put every thing in side to make a /c/tools/hadoop-3.2.0/bin(You might not need to do this, but after testing it without the /bin I could not make it to work)\nThen follow the solution in this video: How To Resolve Issue with Writing DataFrame to Local File | winutils | msvcp100.dll (youtube.com)\nRemember to restart IDE and computer, After the error An error occurred while calling o54.parquet.  is fixed but new errors like o31.parquet. Or o35.parquet. appear.", "section": "Module 5: pyspark", "question": "Java+Spark - Easy setup with miniconda env (worked on MacOS)", "course": "data-engineering-zoomcamp", "document_id": "e86ca928"}