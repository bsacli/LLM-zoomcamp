{"text": "Let\u2019s say we define our Conv2d layer like this:\n>> tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(150, 150, 3))\nIt means our input image is RGB (3 channels, 150 by 150 pixels), kernel is 3x3 and number of filters (layer\u2019s width) is 32.\nIf we check model.summary() we will get this:\n_________________________________________________________________\nLayer (type)                Output Shape              Param #\n=================================================================\nconv2d (Conv2D)             (None, 148, 148, 32)      896\nSo where does 896 params come from? It\u2019s computed like this:\n>>> (3*3*3 +1) * 32\n896\n# 3x3 kernel, 3 channels RGB, +1 for bias, 32 filters\nWhat about the number of \u201cfeatures\u201d we get after the Flatten layer?\nFor our homework model.summary() for last MaxPooling2d and Flatten layers looked like this:\n_________________________________________________________________\nLayer (type)                Output Shape              Param #\n=================================================================\nmax_pooling2d_3       (None, 7, 7, 128)         0\nflatten (Flatten)           (None, 6272)              0\nSo where do 6272 vectors come from? It\u2019s computed like this:\n>>> 7*7*128\n6272\n# 7x7 \u201cimage shape\u201d after several convolutions and poolings, 128 filters\nAdded by Andrii Larkin", "section": "8. Neural Networks and Deep Learning", "question": "Q: Where does the number of Conv2d layer\u2019s params come from? Where does the number of \u201cfeatures\u201d we get after the Flatten layer come from?", "course": "machine-learning-zoomcamp", "document_id": "879c1ec0"}