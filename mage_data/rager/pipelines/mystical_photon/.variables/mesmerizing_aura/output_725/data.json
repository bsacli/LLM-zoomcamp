{"text": "This deployment setup can be tested locally using AWS RIE (runtime interface emulator).\nBasically, if your Docker image was built upon base AWS Lambda image (FROM public.ecr.aws/lambda/python:3.10) - just use certain ports for \u201cdocker run\u201d and a certain \u201clocalhost link\u201d for testing:\ndocker run -it --rm -p 9000:8080 name\nThis command runs the image as a container and starts up an endpoint locally at:\nlocalhost:9000/2015-03-31/functions/function/invocations\nPost an event to the following endpoint using a curl command:\ncurl -XPOST \"http://localhost:9000/2015-03-31/functions/function/invocations\" -d '{}'\nExamples of curl testing:\n* windows testing:\ncurl -XPOST \"http://localhost:9000/2015-03-31/functions/function/invocations\" -d \"{\\\"url\\\": \\\"https://habrastorage.org/webt/rt/d9/dh/rtd9dhsmhwrdezeldzoqgijdg8a.jpeg\\\"}\"\n* unix testing:\ncurl -XPOST \"http://localhost:9000/2015-03-31/functions/function/invocations\" -d '{\"url\": \"https://habrastorage.org/webt/rt/d9/dh/rtd9dhsmhwrdezeldzoqgijdg8a.jpeg\"}'\nIf during testing you encounter an error like this:\n# {\"errorMessage\": \"Unable to marshal response: Object of type float32 is not JSON serializable\", \"errorType\": \"Runtime.MarshalError\", \"requestId\": \"7ea5d17a-e0a2-48d5-b747-a16fc530ed10\", \"stackTrace\": []}\njust turn your response at lambda_handler() to string - str(result).\nAdded by Andrii Larkin", "section": "9. Serverless Deep Learning", "question": "How to test AWS Lambda + Docker locally?", "course": "machine-learning-zoomcamp", "document_id": "0cfbe2e2"}