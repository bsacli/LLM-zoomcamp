{"text": "Prior to using Ollama models in llm-zoomcamp tasks, you need to have ollama installed on your pc and the relevant LLM model downloaded with ollama from https://www.ollama.com\nTo download ollama for Ubuntu:\n``` curl -fsSL https://ollama.com/install.sh | sh ```\nTo download ollama for Mac and Windows, follow the guide on this link:\nhttps://ollama.com/download/\nOllama a number of open-source LLMs like:\nLlama3\nPhi3\nMistral and Mixtral\nGemma\nQwen\nYou can explore more models on https://ollama.com/library/\nTo download a model in Ollama, simply open command prompt and type:\n``` ollama run model_name ```\ne.g.\n``` ollama run phi3 ```\nIt will automatically download the model and you can use it same way as above for later time.\nTo use Ollama models for inference and llm-zoomcamp tasks, use the following function:\nimport ollama\ndef llm(prompt):\nresponse = ollama.chat(\nmodel=\"llama3\",\nmessages=[{\"role\": \"user\", \"content\": prompt}]\n)\nreturn response['message']['content']\nFor example, we can use it in the following way:\nprompt = \"When does the llm-zoomcamp course start?\"\nanswer = llm(prompt)\nprint(answer)", "section": "Module 1: Introduction", "question": "OpenSource: How can I use Ollama open-source models locally on my pc without using any API?", "course": "llm-faq-v1", "document_id": "3d6f30b1"}